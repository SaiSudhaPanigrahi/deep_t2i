# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04a_trainer_DAMSM.ipynb (unless otherwise specified).

__all__ = ['compute_sent_loss', 'compute_word_loss', 'AnimeHeadsTrainer', 'BirdsTrainer']

# Cell
import time
from tqdm.auto import tqdm
from itertools import chain
import torch
import torch.nn as nn
import torch.optim as optim
from kornia.augmentation import RandomHorizontalFlip, RandomCrop
from kornia.geometry.transform import resize
from fastcore.all import *

from .data_anime_heads import Datasets as AnimeHeadsDatasets, DataLoaders as AnimeHeadsDataLoaders
from .data_birds import Datasets as BirdsDatasets, DataLoaders as BirdsDataLoaders
from .model import RnnEncoder, CnnEncoder
from .torch_utils import *

# Internal Cell
ce_loss = nn.CrossEntropyLoss()
random_hflip = RandomHorizontalFlip().requires_grad_(False).eval()
random_crop = RandomCrop((229, 229)).requires_grad_(False).eval()

# Internal Cell
def cosine_similarity(x1, x2, dim=1, eps=1e-8):
    """Returns cosine similarity between x1 and x2, computed along dim.
    """
    w12 = torch.sum(x1 * x2, dim)
    w1 = torch.norm(x1, 2, dim)
    w2 = torch.norm(x2, 2, dim)
    return (w12 / (w1 * w2).clamp(min=eps)).squeeze()
def func_attention(query, context, gamma1):
    """
    query: batch x ndf x queryL
    context: batch x ndf x ih x iw (sourceL=ihxiw)
    mask: batch_size x sourceL
    """
    batch_size, queryL = query.size(0), query.size(2)
    ih, iw = context.size(2), context.size(3)
    sourceL = ih * iw

    # --> batch x sourceL x ndf
    context = context.view(batch_size, -1, sourceL)
    contextT = torch.transpose(context, 1, 2).contiguous()

    # Get attention
    # (batch x sourceL x ndf)(batch x ndf x queryL)
    # -->batch x sourceL x queryL
    attn = torch.bmm(contextT, query) # Eq. (7) in AttnGAN paper
    # --> batch*sourceL x queryL
    attn = attn.view(batch_size*sourceL, queryL)
    attn = torch.softmax(attn, 1)  # Eq. (8)

    # --> batch x sourceL x queryL
    attn = attn.view(batch_size, sourceL, queryL)
    # --> batch*queryL x sourceL
    attn = torch.transpose(attn, 1, 2).contiguous()
    attn = attn.view(batch_size*queryL, sourceL)
    #  Eq. (9)
    attn = attn * gamma1
    attn = torch.softmax(attn, 1)
    attn = attn.view(batch_size, queryL, sourceL)
    # --> batch x sourceL x queryL
    attnT = torch.transpose(attn, 1, 2).contiguous()

    # (batch x ndf x sourceL)(batch x sourceL x queryL)
    # --> batch x ndf x queryL
    weightedContext = torch.bmm(context, attnT)

    return weightedContext, attn.view(batch_size, -1, ih, iw)

# Cell
def compute_sent_loss(cnn_code, sent_emb, gamma3=10.0, eps=1e-8):
    ''' cnn_code: (bs, emb_sz), sent_emb: (bs, emb_sz) '''
    bs = cnn_code.shape[0]
    device = cnn_code.device
    labels = torch.tensor(range(bs), device=device)
    # --> seq_len x batch_size x embrace_sz
    if cnn_code.dim() == 2:
        cnn_code = cnn_code.unsqueeze(0)
        sent_emb = sent_emb.unsqueeze(0)

    # cnn_code_norm / sent_emb: seq_len x batch_size x 1
    cnn_code_norm = torch.norm(cnn_code, 2, dim=2, keepdim=True)
    sent_emb_norm = torch.norm(sent_emb, 2, dim=2, keepdim=True)
    # scores* / norm*: seq_len x batch_size x batch_size
    scores0 = torch.bmm(cnn_code, sent_emb.transpose(1, 2))
    norm0 = torch.bmm(cnn_code_norm, sent_emb_norm.transpose(1, 2))
    scores0 = scores0 / norm0.clamp(min=eps) * gamma3

    # --> batch_size x batch_size
    scores0 = scores0.squeeze()
    scores1 = scores0.transpose(0, 1)

    loss0 = ce_loss(scores0, labels)
    loss1 = ce_loss(scores1, labels)

    return loss0, loss1

# Cell
def compute_word_loss(img_features, words_emb, cap_len, gamma1=4.0, gamma2=5.0, gamma3=10.0):
    """
        img_features(context): batch x emb_sz x 17 x 17
        words_emb(query): batch x seq_len x emb_sz
    """
    words_emb = words_emb.permute(0, 2, 1) # batch x emb_sz x seq_len
    bs = img_features.shape[0]
    device = img_features.device
    labels = torch.tensor(range(bs), device=device)

    att_maps = []
    similarities = []
    cap_len = cap_len.data.tolist()
    for i in range(bs):
        # Get the i-th text description
        words_num = cap_len[i]
        # -> 1 x emb_sz x words_num
        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()
        # -> batch_size x emb_sz x words_num
        word = word.repeat(bs, 1, 1)
        # batch x emb_sz x 17*17
        context = img_features
        """
            word(query): batch x emb_sz x words_num
            context: batch x emb_sz x 17 x 17
            weiContext: batch x emb_sz x words_num
            attn: batch x words_num x 17 x 17
        """
        weiContext, attn = func_attention(word, context, gamma1)
        att_maps.append(attn[i].contiguous())
        # --> batch_size x words_num x emb_sz
        word = word.transpose(1, 2).contiguous()
        weiContext = weiContext.transpose(1, 2).contiguous()
        # --> batch_size*words_num x emb_sz
        word = word.view(bs * words_num, -1)
        weiContext = weiContext.view(bs * words_num, -1)
        #
        # -->batch_size*words_num
        row_sim = cosine_similarity(word, weiContext)
        # --> batch_size x words_num
        row_sim = row_sim.view(bs, words_num)

        # Eq. (10)
        row_sim.mul_(gamma2).exp_()
        row_sim = row_sim.sum(dim=1, keepdim=True)
        row_sim = torch.log(row_sim)

        # --> 1 x batch_size
        # similarities(i, j): the similarity between the i-th image and the j-th text description
        similarities.append(row_sim)

    # batch_size x batch_size
    similarities = torch.cat(similarities, 1)

    similarities = similarities * gamma3
    similarities1 = similarities.transpose(0, 1)

    loss0 = ce_loss(similarities, labels)
    loss1 = ce_loss(similarities1, labels)

    return loss0, loss1, att_maps

# Cell
class AnimeHeadsTrainer():
    def __init__(
        self,
        data_dir,
        bs,
        data_pct=1,
        lr=3e-3,
        lr_decay=0.98,
        device='cpu',
        # model params
        emb_sz=24,
        rnn_layers=2,
        rnn_drop_p=0.5,
        gamma1=4.0,
        gamma2=5.0,
        gamma3=10.0,
    ):
        super().__init__()
        self.emb_sz = emb_sz
        self.rnn_layers = rnn_layers
        self.rnn_drop_p = rnn_drop_p
        self.gamma1 = gamma1
        self.gamma2 = gamma2
        self.gamma3 = gamma3

        self.device = torch.device(device)
        self.normalizer = Normalizer(device=self.device)

        dsets = AnimeHeadsDatasets(data_dir, pct=data_pct, valid_pct=0)
        self.dls = AnimeHeadsDataLoaders(dsets, bs)
        self.vocab_sz = dsets.train.tokenizer.vocab_sz
        self.pad_id = dsets.train.tokenizer.pad_id
        self.max_seq_len = dsets.train.tokenizer.max_seq_len

        self.rnn_encoder = RnnEncoder(self.vocab_sz, self.emb_sz, self.pad_id, n_layers=self.rnn_layers, drop_p=self.rnn_drop_p).to(device)
        self.cnn_encoder = CnnEncoder(self.emb_sz).to(device)
        self.optim = optim.Adam(
            chain(self.rnn_encoder.parameters(), self.cnn_encoder.emb_features.parameters(), self.cnn_encoder.emb_cnn_code.parameters()),
            lr=lr, betas=(0.5, 0.999)
        )
        self.lr_scheduler = optim.lr_scheduler.ExponentialLR(self.optim, lr_decay)
    def after_batch_tfm(self, cap, cap_len, img):
        " cap: (bs, max_seq_len), cap_len: (bs,), img: (bs, 64, 64, 3) "
        img = img.permute(0, 3, 1, 2).float() # (bs, 3, 64, 64)
        img = random_hflip(img)
        img = resize(img, size=(229, 229)) # (bs, 3, 229, 229)
        img = self.normalizer.encode(img)
        return cap, cap_len, img

# Cell
class BirdsTrainer():
    def __init__(
        self,
        data_dir,
        bs,
        data_pct=1,
        lr=3e-3,
        lr_decay=0.98,
        device='cpu',
        # model params
        emb_sz=256,
        rnn_layers=2,
        rnn_drop_p=0.5,
        gamma1=4.0,
        gamma2=5.0,
        gamma3=10.0,
    ):
        super().__init__()
        self.emb_sz = emb_sz
        self.rnn_layers = rnn_layers
        self.rnn_drop_p = rnn_drop_p
        self.gamma1 = gamma1
        self.gamma2 = gamma2
        self.gamma3 = gamma3

        self.device = torch.device(device)
        self.normalizer = Normalizer(device=self.device)

        dsets = BirdsDatasets(data_dir, pct=data_pct)
        self.dls = BirdsDataLoaders(dsets, bs)
        self.vocab_sz = dsets.train.tokenizer.vocab_sz
        self.pad_id = dsets.train.tokenizer.pad_id
        self.max_seq_len = dsets.train.tokenizer.max_seq_len

        self.rnn_encoder = RnnEncoder(self.vocab_sz, self.emb_sz, self.pad_id, n_layers=rnn_layers, drop_p=self.rnn_drop_p).to(device)
        self.cnn_encoder = CnnEncoder(self.emb_sz).to(device)
        self.optim = optim.Adam(
            chain(self.rnn_encoder.parameters(), self.cnn_encoder.emb_features.parameters(), self.cnn_encoder.emb_cnn_code.parameters()),
            lr=lr, betas=(0.5, 0.999)
        )
        self.lr_scheduler = optim.lr_scheduler.ExponentialLR(self.optim, lr_decay)
    def after_batch_tfm(self, cap, cap_len, img):
        " cap: (bs, max_seq_len), cap_len: (bs,), img: (bs, 256, 256, 3) "
        img = img.permute(0, 3, 1, 2).float() # (bs, 3, 256, 256)
        img = resize(img, size=(int(256 * 76 / 64), int(256 * 76 / 64))) # (bs, 3, 304, 304)
        img = random_crop(img) # (bs, 3, 229, 229)
        img = random_hflip(img)
        img = self.normalizer.encode(img)
        return cap, cap_len, img

# Internal Cell
@patch
@torch.no_grad()
def fig_for_show(self: [AnimeHeadsTrainer, BirdsTrainer], is_valid=False):
    dl = self.dls.valid if is_valid else self.dls.train
    cap, cap_len, img = to_device(iter(dl).next(), device=self.device)
    cap, cap_len, img = self.after_batch_tfm(cap, cap_len, img)
    self.cnn_encoder.eval()
    self.rnn_encoder.eval()
    word_features, cnn_code = self.cnn_encoder(img)
    sent_emb, word_emb = self.rnn_encoder(cap, cap_len)
    w_loss0, w_loss1, attn_maps = compute_word_loss(word_features, word_emb, cap_len) # list of (true_seqlen, 17, 17)

    cap_len = cap_len[0].cpu().item()
    cap = [self.dls.dsets.train.tokenizer.decode([cap[0][i].cpu()]) for i in range(cap_len)] # 'one cap'
    img = self.normalizer.decode(img).permute(0, 2, 3, 1).cpu()[0] # (229, 229, 3)
    attn_map = attn_maps[0].cpu() # (true_seqlen, 17, 17)

    ncols = 4
    nrows = math.ceil((cap_len+1)/ncols)
    figsize = (ncols*4, nrows*4)
    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)
    axs = ax.flatten()
    axs[0].imshow(img)

    for i in range(cap_len):
        axs[i+1].set_title(cap[i])
        axs[i+1].imshow(attn_map[i].cpu())
    return fig
@patch
def show(self: [AnimeHeadsTrainer, BirdsTrainer], is_valid=False):
    fig = self.fig_for_show(is_valid)
    display(fig)
    plt.close()

# Internal Cell
@patch
def save_checkpoint(self: [AnimeHeadsTrainer, BirdsTrainer], path):
    state = {
        'rnn_encoder': self.rnn_encoder.state_dict(),
        'cnn_encoder.emb_features': self.cnn_encoder.emb_features.state_dict(),
        'cnn_encoder.emb_cnn_code': self.cnn_encoder.emb_cnn_code.state_dict(),
        'optim': self.optim.state_dict(),
        'lr_scheduler': self.lr_scheduler.state_dict(),
    }
    torch.save(state, path)
@patch
def load_checkpoint(self: [AnimeHeadsTrainer, BirdsTrainer], path):
    state = torch.load(path, map_location=self.device)
    self.rnn_encoder.load_state_dict(state['rnn_encoder'])
    self.cnn_encoder.emb_features.load_state_dict(state['cnn_encoder.emb_features'])
    self.cnn_encoder.emb_cnn_code.load_state_dict(state['cnn_encoder.emb_cnn_code'])
    self.optim.load_state_dict(state['optim'])
    self.lr_scheduler.load_state_dict(state['lr_scheduler'])
@patch
def export(self: [AnimeHeadsTrainer, BirdsTrainer], path: Path):
    state = {
        'vocab_sz': self.vocab_sz,
        'emb_sz': self.emb_sz,
        'pad_id': self.pad_id,
        'rnn_layers': self.rnn_layers,
        'rnn_drop_p': self.rnn_drop_p,
        'rnn_encoder': self.rnn_encoder.state_dict(),
        'cnn_encoder.emb_features': self.cnn_encoder.emb_features.state_dict(),
        'cnn_encoder.emb_cnn_code': self.cnn_encoder.emb_cnn_code.state_dict(),
        'gamma1': self.gamma1,
        'gamma2': self.gamma2,
        'gamma3': self.gamma3,
    }
    torch.save(state, path)

# Internal Cell
@patch
def train(
    self: [AnimeHeadsTrainer, BirdsTrainer],
    n_step,
    step_per_epoch=500, # print loss each step_per_epoch
    saveck_every=None,
    ck_path:str=None,
):
    assert n_step%step_per_epoch==0, f'n_step: {n_step} % step_per_epoch: {step_per_epoch} should be 0'
    losses = []
    total_start_t = time.time()
    log_start_t = time.time()
    dl = InfiniteDl(self.dls.train)

    pb = tqdm(range(1, n_step+1))
    for step in pb:
        self.rnn_encoder.train()
        self.cnn_encoder.train()
        self.optim.zero_grad()

        cap, cap_len, img = to_device(dl.next(), device=self.device)
        cap, cap_len, img = self.after_batch_tfm(cap, cap_len, img)
        word_features, cnn_code = self.cnn_encoder(img)
        sent_emb, word_emb = self.rnn_encoder(cap, cap_len)

        s_loss0, s_loss1 = compute_sent_loss(cnn_code, sent_emb, gamma3=self.gamma3)
        w_loss0, w_loss1, attn_maps = compute_word_loss(word_features, word_emb, cap_len, gamma1=self.gamma1, gamma2=self.gamma2, gamma3=self.gamma3)
        loss = s_loss0+s_loss1+w_loss0+w_loss1
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.rnn_encoder.parameters(), 0.25)
        self.optim.step()

        losses.append(loss.detach().cpu())
        pb.set_postfix(loss=loss.detach().cpu().numpy())


        if saveck_every and step%saveck_every == 0:
            self.save_checkpoint(path=ck_path+f'-{step//saveck_every}.pt')
        if step%step_per_epoch == 0: # Each epoch
            if self.lr_scheduler.get_last_lr()[0] > self.lr_scheduler.base_lrs[0]/10:
                self.lr_scheduler.step()
#             print(self.lr_scheduler.get_last_lr()[0], self.lr_scheduler.base_lrs[0])
            duration = time.time() - log_start_t
            msg = f'{step//step_per_epoch}, time: {duration:.1f}s, loss: {torch.tensor(losses).mean():.4f}'
            tqdm.write(msg)
            losses = []
            log_start_t = time.time()

    pb.close()
    tqdm.write(f'total_time: {(time.time()-total_start_t)/60:.1f}min')