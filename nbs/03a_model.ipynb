{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from transformers import AutoModel\n",
    "from fastcore.all import *\n",
    "\n",
    "from deep_t2i.torch_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAMSM and Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class RnnEncoder(nn.Module):\n",
    "    def __init__(self, vocab_sz, emb_sz, pad_id, n_layers=2, drop_p=0.5):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.emb_sz = emb_sz\n",
    "        self.pad_id = pad_id\n",
    "        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_id)\n",
    "        self.emb.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.gru = nn.GRU(emb_sz, emb_sz//2, num_layers=n_layers, batch_first=True, dropout=drop_p, bidirectional=True)\n",
    "    def forward(self, inp_ids, true_seqlen):\n",
    "        ''' inp_ids: (bs, seq_len), true_seqlen: (bs,)\n",
    "            returns: (bs, emb_sz), (bs, seq_len, emb_sz) '''\n",
    "        bs, seq_len = inp_ids.shape\n",
    "        emb = self.emb(inp_ids) # (bs, seq_len, emb_sz)\n",
    "        emb_packed = pack_padded_sequence(emb, true_seqlen, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        word_emb_packed, hn = self.gru(emb_packed) # hn: (n_layers*2, bs, emb_sz//2)\n",
    "        word_emb, _ = pad_packed_sequence(word_emb_packed, batch_first=True, total_length=seq_len) # (bs, seq_len, emb_sz)\n",
    "\n",
    "        hn = hn.view(self.n_layers, 2, bs, self.emb_sz//2) # (self.n_layers, 2, bs, self.emb_sz//2)\n",
    "        sent_emb = torch.cat([hn[-1][0], hn[-1][1]], 1)\n",
    "        return sent_emb, word_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = RnnEncoder(vocab_sz=111, emb_sz=400, pad_id=1)\n",
    "inp_ids = torch.randint(0, 100, (16, 20))\n",
    "true_seqlen = torch.randint(1, 20, (16,))\n",
    "sent_emb, word_emb = encoder(inp_ids, true_seqlen)\n",
    "test_eq(sent_emb.shape, (16, 400))\n",
    "test_eq(word_emb.shape, (16, 20, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CnnEncoder(nn.Module):\n",
    "    def __init__(self, emb_sz):\n",
    "        ''' from https://github.com/taoxugit/AttnGAN/blob/master/code/model.py '''\n",
    "        super().__init__()\n",
    "        self.emb_sz = emb_sz\n",
    "        model = torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', pretrained=True, verbose=False).requires_grad_(False).eval()\n",
    "        \n",
    "        self.define_module(model)\n",
    "        self.init_trainable_weights()\n",
    "\n",
    "    def define_module(self, model):\n",
    "        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n",
    "        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n",
    "        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n",
    "        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n",
    "        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n",
    "        self.Mixed_5b = model.Mixed_5b\n",
    "        self.Mixed_5c = model.Mixed_5c\n",
    "        self.Mixed_5d = model.Mixed_5d\n",
    "        self.Mixed_6a = model.Mixed_6a\n",
    "        self.Mixed_6b = model.Mixed_6b\n",
    "        self.Mixed_6c = model.Mixed_6c\n",
    "        self.Mixed_6d = model.Mixed_6d\n",
    "        self.Mixed_6e = model.Mixed_6e\n",
    "        self.Mixed_7a = model.Mixed_7a\n",
    "        self.Mixed_7b = model.Mixed_7b\n",
    "        self.Mixed_7c = model.Mixed_7c\n",
    "\n",
    "        self.emb_features = nn.Conv2d(768, self.emb_sz, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.emb_cnn_code = nn.Linear(2048, self.emb_sz)\n",
    "        \n",
    "        self.upsample = nn.Upsample(size=(299, 299), mode='bilinear', align_corners=False)\n",
    "\n",
    "    def init_trainable_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.emb_features.weight.data.uniform_(-initrange, initrange)\n",
    "        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' x: (bs, 3, 299, 299) \n",
    "            returns: features: (bs, emb_sz, 17, 17), cnn_code: (bs, emb_sz)'''\n",
    "        features = None\n",
    "        # --> fixed-size input: batch x 3 x 299 x 299\n",
    "        x = self.upsample(x)\n",
    "        # 299 x 299 x 3\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # 149 x 149 x 32\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # 147 x 147 x 32\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # 147 x 147 x 64\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # 73 x 73 x 64\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        # 73 x 73 x 80\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        # 71 x 71 x 192\n",
    "\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # 35 x 35 x 192\n",
    "        x = self.Mixed_5b(x)\n",
    "        # 35 x 35 x 256\n",
    "        x = self.Mixed_5c(x)\n",
    "        # 35 x 35 x 288\n",
    "        x = self.Mixed_5d(x)\n",
    "        # 35 x 35 x 288\n",
    "\n",
    "        x = self.Mixed_6a(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6b(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6c(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6d(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6e(x)\n",
    "        # 17 x 17 x 768\n",
    "\n",
    "        # image region features\n",
    "        features = x\n",
    "        # 17 x 17 x 768\n",
    "\n",
    "        x = self.Mixed_7a(x)\n",
    "        # 8 x 8 x 1280\n",
    "        x = self.Mixed_7b(x)\n",
    "        # 8 x 8 x 2048\n",
    "        x = self.Mixed_7c(x)\n",
    "        # 8 x 8 x 2048\n",
    "        x = F.avg_pool2d(x, kernel_size=8)\n",
    "        # 1 x 1 x 2048\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        # 1 x 1 x 2048\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # 2048\n",
    "\n",
    "        # global image features\n",
    "        cnn_code = self.emb_cnn_code(x)\n",
    "        # 512\n",
    "        if features is not None:\n",
    "            features = self.emb_features(features)\n",
    "        return features, cnn_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/archive/v0.6.0.zip\" to /root/.cache/torch/hub/v0.6.0.zip\n",
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36dd37f1e5a4de4b1e37c216fb921b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=108857766.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_encoder = CnnEncoder(256)\n",
    "x = torch.randn(2, 3, 229, 229)\n",
    "features, cnn_code = cnn_encoder(x)\n",
    "test_eq(features.shape, (2, 256, 17, 17))\n",
    "test_eq(cnn_code.shape, (2, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_pretrained_DAMSM(path, device='cpu'):\n",
    "    ''' returns: rnn_encoder, cnn_encoder, gamma1, gamma2, gamma3 '''\n",
    "    state = torch.load(path, map_location=device)\n",
    "    vocab_sz = state['vocab_sz']\n",
    "    emb_sz = state['emb_sz']\n",
    "    pad_id = state['pad_id']\n",
    "    rnn_layers = state['rnn_layers']\n",
    "    rnn_drop_p = state['rnn_drop_p']\n",
    "    rnn_encoder = RnnEncoder(vocab_sz, emb_sz, pad_id, n_layers=rnn_layers).to(device)\n",
    "    rnn_encoder.load_state_dict(state['rnn_encoder'])\n",
    "    \n",
    "    cnn_encoder = CnnEncoder(emb_sz).to(device)\n",
    "    cnn_encoder.emb_features.load_state_dict(state['cnn_encoder.emb_features'])\n",
    "    cnn_encoder.emb_cnn_code.load_state_dict(state['cnn_encoder.emb_cnn_code'])\n",
    "    \n",
    "    gamma1 = state['gamma1']\n",
    "    gamma2 = state['gamma2']\n",
    "    gamma3 = state['gamma3']\n",
    "    \n",
    "    return rnn_encoder, cnn_encoder, vocab_sz, emb_sz, pad_id, rnn_layers, rnn_drop_p, gamma1, gamma2, gamma3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "def conv2d(ni, nf, ks, stride, padding, bias=True):\n",
    "    \" conv2d with kaiming init \"\n",
    "    conv = nn.Conv2d(ni, nf, ks, stride, padding, bias=bias)\n",
    "    nn.init.kaiming_normal_(conv.weight)\n",
    "    return conv\n",
    "def spectral_conv2d(ni, nf, ks, stride, padding, bias=True):\n",
    "    \" conv2d with spectral norm \"\n",
    "    return nn.utils.spectral_norm(conv2d(ni, nf, ks, stride, padding, bias=bias))\n",
    "class MinibatchStdDev(nn.Module):\n",
    "    \"\"\"\n",
    "    Minibatch standard deviation layer for the discriminator, try to prevent mode collapse\n",
    "    From https://github.com/akanimax/BMSG-GAN/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        derived class constructor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, alpha=1e-8):\n",
    "        \"\"\"\n",
    "        forward pass of the layer\n",
    "        :param x: input activation volume\n",
    "        :param alpha: small number for numerical stability\n",
    "        :return: y => x appended with standard deviation constant map\n",
    "        \"\"\"\n",
    "        batch_size, _, height, width = x.shape\n",
    "\n",
    "        # [B x C x H x W] Subtract mean over batch.\n",
    "        y = x - x.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # [1 x C x H x W]  Calc standard deviation over batch\n",
    "        y = torch.sqrt(y.pow(2.).mean(dim=0, keepdim=False) + alpha)\n",
    "\n",
    "        # [1]  Take average over feature_maps and pixels.\n",
    "        y = y.mean().view(1, 1, 1, 1)\n",
    "\n",
    "        # [B x 1 x H x W]  Replicate over group and pixels.\n",
    "        y = y.repeat(batch_size, 1, height, width)\n",
    "\n",
    "        # [B x C x H x W]  Append as new feature_map.\n",
    "        y = torch.cat([x, y], 1)\n",
    "\n",
    "        # return the computed values:\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = spectral_conv2d(64, 32, 3, 1, 1)\n",
    "inp = torch.randn(2, 64, 16, 16)\n",
    "out = conv(inp)\n",
    "test_eq(out.shape, (2, 32, 16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "def conv_block_g(ni, nf, ks=3, stride=1, padding=1):\n",
    "    \" spectral_conv2d + batchnorm2d + 0.2 leakyReLU \"\n",
    "    return nn.Sequential(\n",
    "        spectral_conv2d(ni, nf, ks, stride, padding, bias=False),\n",
    "        nn.BatchNorm2d(nf),\n",
    "        nn.LeakyReLU(0.2),\n",
    "    )\n",
    "class ConvResBlockG(nn.Module):\n",
    "    def __init__(self, nf):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            conv_block_g(nf, nf),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \" x: (bs, nf, _, _), returns: (bs, nf, _, _) \"\n",
    "        out = self.conv(x)\n",
    "        return x + out\n",
    "def conv_block_d(ni, nf, ks=3, stride=1, padding=1):\n",
    "    \" spectral_conv2d + 0.2 leakyReLU \"\n",
    "    return nn.Sequential(\n",
    "        spectral_conv2d(ni, nf, ks, stride, padding, bias=True),\n",
    "        nn.LeakyReLU(0.2),\n",
    "    )\n",
    "class ConvResBlockD(nn.Module):\n",
    "    def __init__(self, nf):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            conv_block_d(nf, nf),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \" x: (bs, nf, _, _), returns: (bs, nf, _, _) \"\n",
    "        out = self.conv(x)\n",
    "        return x + out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = conv_block_g(64, 32, 3, 1, 1)\n",
    "inp = torch.randn(2, 64, 16, 16)\n",
    "out = conv(inp)\n",
    "test_eq(out.shape, (2, 32, 16, 16))\n",
    "\n",
    "conv = conv_block_d(64, 32, 3, 1, 1)\n",
    "inp = torch.randn(2, 64, 16, 16)\n",
    "out = conv(inp)\n",
    "test_eq(out.shape, (2, 32, 16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "def up_block():\n",
    "    ''' (bs, _, _, _) -> (bs, _, _*2, _*2) '''\n",
    "    return nn.Upsample(scale_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up = up_block()\n",
    "inp = torch.randn(2, 3, 16, 16)\n",
    "out = up(inp)\n",
    "test_eq(out.shape, (2, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "def down_block():\n",
    "    ''' (bs, _, _, _) -> (bs, _, _//2, _//2) '''\n",
    "    return nn.AvgPool2d(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down = down_block()\n",
    "inp = torch.randn(2, 3, 32, 32)\n",
    "out = down(inp)\n",
    "test_eq(out.shape, (2, 3, 16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "def to_rgb_block(ni):\n",
    "    ''' (bs, ni, _, _) -> (bs, 3, _, _) '''\n",
    "    return nn.Sequential(\n",
    "        conv2d(ni, 3, 1, 1, 0, bias=False),\n",
    "        nn.Tanh()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_rbg = to_rgb_block(64)\n",
    "inp = torch.randn(2, 64, 8, 8)\n",
    "out = to_rbg(inp)\n",
    "test_eq(out.shape, (2, 3, 8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "def from_rgb_block(nf):\n",
    "    ''' (bs, 3, _, _) -> (bs, nf, _, _) '''\n",
    "    return nn.Sequential(\n",
    "        spectral_conv2d(3, nf, 1, 1, 0, bias=True),\n",
    "        nn.LeakyReLU(0.2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_rbg = from_rgb_block(64)\n",
    "inp = torch.randn(2, 3, 8, 8)\n",
    "out = from_rbg(inp)\n",
    "test_eq(out.shape, (2, 64, 8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "class SelfAttention(nn.Module):\n",
    "    ''' expensive operator, usually be used on 64x64 layer, better to not use on layers that larger than 64x64 if you don't have a large GPU\n",
    "        from https://github.com/ajbrock/BigGAN-PyTorch '''\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        # Channel multiplier\n",
    "        self.ch = ch\n",
    "        self.theta = spectral_conv2d(self.ch, self.ch // 8, 1, 1, 0, bias=False)\n",
    "        self.phi = spectral_conv2d(self.ch, self.ch // 8, 1, 1, 0, bias=False)\n",
    "        self.g = spectral_conv2d(self.ch, self.ch // 2, 1, 1, 0, bias=False)\n",
    "        self.o = spectral_conv2d(self.ch // 2, self.ch, 1, 1, 0, bias=False)\n",
    "        # Learnable gain parameter\n",
    "        self.gamma = nn.Parameter(torch.tensor(0.))\n",
    "    def forward(self, x):\n",
    "        # Apply convs\n",
    "        theta = self.theta(x)\n",
    "        phi = F.max_pool2d(self.phi(x), [2,2])\n",
    "        g = F.max_pool2d(self.g(x), [2,2])    \n",
    "        # Perform reshapes\n",
    "        theta = theta.view(-1, self. ch // 8, x.shape[2] * x.shape[3])\n",
    "        phi = phi.view(-1, self. ch // 8, x.shape[2] * x.shape[3] // 4)\n",
    "        g = g.view(-1, self. ch // 2, x.shape[2] * x.shape[3] // 4)\n",
    "        # Matmul and softmax to get attention maps\n",
    "        beta = F.softmax(torch.bmm(theta.transpose(1, 2), phi), -1)\n",
    "        # Attention map times g path\n",
    "        o = self.o(torch.bmm(g, beta.transpose(1,2)).view(-1, self.ch // 2, x.shape[2], x.shape[3]))\n",
    "        return self.gamma * o + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attn = SelfAttention(16)\n",
    "x = torch.randn(2, 16, 4, 4)\n",
    "out = self_attn(x)\n",
    "test_eq(out.shape, (2, 16, 4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "def simple_attn(tgt, src, src_mask=None):\n",
    "    ''' tgt: (bs, tgt_seq_len, emb_sz), src: (bs, src_seq_len, emb_sz), src_mask: (bs, src_seq_len) [True will be masked]\n",
    "        returns: (bs, tgt_seq_len, emb_sz), (bs, tgt_seq_len, src_seq_len) '''\n",
    "    bs, tgt_seq_len, emb_sz = tgt.shape\n",
    "    _, src_seq_len, _ = src.shape\n",
    "    \n",
    "    attn_w = torch.bmm(tgt, src.permute(0, 2, 1)) # (bs, tgt_seq_len, src_seq_len)\n",
    "#     print(attn_w.view(bs*tgt_seq_len, src_seq_len))\n",
    "    if src_mask is not None: attn_w.masked_fill_(src_mask.unsqueeze(1), -float('inf')) # (bs, tgt_seq_len, src_seq_len)\n",
    "    attn_w = torch.softmax(attn_w, 2) # (bs, tgt_seq_len, src_seq_len)\n",
    "    attn_out = torch.bmm(attn_w, src) # (bs, tgt_seq_len, emb_sz)\n",
    "\n",
    "    return attn_out, attn_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # exporti\n",
    "# def simple_attn2(tgt, src, src_mask=None):\n",
    "#     ''' tgt: (bs, tgt_seq_len, emb_sz), src: (bs, src_seq_len, emb_sz), src_mask: (bs, src_seq_len) [True will be masked]\n",
    "#         returns: (bs, tgt_seq_len, emb_sz), (bs, tgt_seq_len, src_seq_len) '''\n",
    "#     bs, tgt_seq_len, emb_sz = tgt.shape\n",
    "#     _, src_seq_len, _ = src.shape\n",
    "#     src = src.permute(0, 2, 1) # (bs, emb_sz, src_seq_len)\n",
    "#     attn_w = torch.bmm(tgt, src) # (bs, tgt_seq_len, src_seq_len)\n",
    "#     attn_w = attn_w.view(bs*tgt_seq_len, src_seq_len) # (bs*tgt_seq_len, src_seq_len)\n",
    "    \n",
    "#     if src_mask is not None:\n",
    "#         mask = src_mask.repeat(tgt_seq_len, 1) # (bs*tgt_seq_len, src_seq_len)\n",
    "#         attn_w.masked_fill_(mask, -float('inf'))\n",
    "    \n",
    "#     attn_w = nn.functional.softmax(attn_w, dim=1)\n",
    "#     attn_w = attn_w.view(bs, tgt_seq_len, src_seq_len)\n",
    "#     attn_w = torch.transpose(attn_w, 1, 2).contiguous() # (bs, src_seq_len, tgt_seq_len)\n",
    "#     attn_out = torch.bmm(src, attn_w) # (bs, emb_sz, tgt_seq_len)\n",
    "#     attn_out = attn_out.permute(0, 2, 1) # (bs, tgt_seq_len, emb_sz)\n",
    "#     attn_w = attn_w.permute(0, 2, 1) # (bs, tgt_seq_len, src_seq_len)\n",
    "\n",
    "#     return attn_out, attn_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = torch.randn(2, 5, 3)\n",
    "src = torch.randn(2, 4, 3)\n",
    "src_mask = torch.tensor([[True, False, False, False],\n",
    "                         [True, True, True, False]])\n",
    "attn_out, attn_w = simple_attn(tgt, src, src_mask)\n",
    "test_eq(attn_out.shape, (2, 5, 3))\n",
    "test_eq(attn_w.shape, (2, 5, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, emb_sz, nc):\n",
    "        super().__init__()\n",
    "        self.emb_sz = emb_sz\n",
    "        self.nc = nc\n",
    "        self.conv1 = conv_block_g(emb_sz, nc, 1, 1, 0)\n",
    "    def forward(self, x, word_emb, src_mask):\n",
    "        \"\"\" Attention from x to word\n",
    "            x: (bs, nc, h, w), word_emb: (bs, seq_len, emb_sz), src_mask: (bs, seq_len)\n",
    "            out: (bs, nc, h, w), self.attn_w: (bs, h, w, seq_len) \"\"\"\n",
    "        bs, nc, h, w = x.shape\n",
    "        _, _, emb_sz = word_emb.shape\n",
    "        assert (nc, emb_sz) == (self.nc, self.emb_sz)\n",
    "        tgt = x.view(bs, nc, -1).permute(0, 2, 1).contiguous() # (bs, h*w, nc)\n",
    "        \n",
    "        # Use Conv1x1 to reduce word_emb dim, Do Not Use Linear!!!\n",
    "        word_emb_t = word_emb.permute(0, 2, 1).unsqueeze(3).contiguous() # (bs, emb_sz, seq_len, 1)\n",
    "        src = self.conv1(word_emb_t).squeeze(3) # (bs, nc, seq_len)\n",
    "        src = src.permute(0, 2, 1).contiguous() # (bs, seq_len, nc)\n",
    "        \n",
    "        attn_out, attn_w = simple_attn(tgt, src, src_mask=src_mask) # (bs, h*w, nc), (bs, h*w, seq_len)\n",
    "        attn_out = attn_out.permute(0, 2, 1).view(bs, nc, h, w) # (bs, nc, h, w)\n",
    "        self.attn_w = attn_w.view(bs, h, w, -1) # (bs, h, w, seq_len)\n",
    "        \n",
    "        out = x + attn_out # (bs, nc, h, w)\n",
    "#         out = torch.cat([x, attn_out], dim=1) # (bs, 2*nc, h, w)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_block = AttnBlock(600, 256)\n",
    "x = torch.randn(16, 256, 16, 16)\n",
    "word_emb = torch.randn(16, 20, 600)\n",
    "src_mask = torch.ones(16, 20).bool()\n",
    "out = attn_block(x, word_emb, src_mask)\n",
    "test_eq(out.shape, (16, 256, 16, 16))\n",
    "test_eq(attn_block.attn_w.shape, (16, 16, 16, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "\n",
    "# CH_TABLE = [512, 512, 512, 512, 256, 128,  64,  32,  16]\n",
    "#             4      8   16   32   64  128  256  512 1024\n",
    "\n",
    "CH_TABLE = [1024, 512, 256, 128,  64,  64,  64,  64,  64]\n",
    "#              4    8   16   32   64  128  256  512 1024\n",
    "\n",
    "# CH_TABLE = [512, 384, 256, 192, 128,  96,  64,  64,  64]\n",
    "#             4    8   16   32   64  128  256  512 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "class G_Init(nn.Module):\n",
    "    def __init__(self, emb_sz, noise_sz=100):\n",
    "        super().__init__()\n",
    "        self.noise_sz = noise_sz\n",
    "        self.conv = nn.Sequential(\n",
    "            conv_block_g(emb_sz+noise_sz, CH_TABLE[0], 4, 1, 3), # (bs, CH_TABLE[0], 4, 4)\n",
    "            ConvResBlockG(CH_TABLE[0]), # (bs, CH_TABLE[0], 4, 4)\n",
    "        )\n",
    "        self.to_rgb = to_rgb_block(CH_TABLE[0]) # (bs, 3, 4, 4)\n",
    "    def forward(self, sent_emb, noise):\n",
    "        ''' sent_emb: (bs, emb_sz), noise: (bs, noise_sz)\n",
    "            returns: img(bs, 3, 4, 4), code(bs, 512, 4, 4) '''\n",
    "        assert noise.shape[1]==self.noise_sz\n",
    "        code = torch.cat([sent_emb, noise], 1)[..., None, None] # (bs, emb_sz+noise_sz, 1, 1)\n",
    "        code = self.conv(code) # (bs, CH_TABLE[0], 4, 4)\n",
    "        img = self.to_rgb(code) # (bs, 3, 4, 4)\n",
    "        return img, code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_init = G_Init(25, 100)\n",
    "sent_emb = torch.randn(2, 25)\n",
    "noise = torch.randn(2, 100)\n",
    "img, code = g_init(sent_emb, noise)\n",
    "test_eq(img.shape, (2, 3, 4, 4))\n",
    "test_eq(code.shape, (2, CH_TABLE[0], 4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "class G_General(nn.Module):\n",
    "    def __init__(self, ni, nf, is_self_attn=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            up_block(),\n",
    "            conv_block_g(ni, nf), # (bs, nf, inp_sz*2, inp_sz*2)\n",
    "            ConvResBlockG(nf), # (bs, nf, inp_sz*2, inp_sz*2)\n",
    "            SelfAttention(nf) if is_self_attn else IdentityModule(), # (bs, nf, inp_sz*2, inp_sz*2)\n",
    "        )\n",
    "        self.to_rgb = to_rgb_block(nf) # (bs, nf, inp_sz*2, inp_sz*2)\n",
    "    def forward(self, code):\n",
    "        ''' code: (bs, ni, inp_sz, inp_sz)\n",
    "            returns: img(bs, 3, inp_sz*2, inp_sz*2), code(bs, nf, inp_sz*2, inp_sz*2) '''\n",
    "        code = self.conv(code) # (bs, nf, inp_sz*2, inp_sz*2)\n",
    "        img = self.to_rgb(code) # (bs, 3, inp_sz*2, inp_sz*2)\n",
    "        return img, code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_general = G_General(512, 256, is_self_attn=True)\n",
    "code = torch.randn(2, 512, 4, 4)\n",
    "img, code = g_general(code)\n",
    "test_eq(img.shape, (2, 3, 8, 8))\n",
    "test_eq(code.shape, (2, 256, 8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "class G_General_Attn(nn.Module):\n",
    "    def __init__(self, emb_sz, ni, nf, is_self_attn=False):\n",
    "        super().__init__()\n",
    "        self.conv = MultiSequential(\n",
    "            AttnBlock(emb_sz, ni), # (bs, ni, inp_sz, inp_sz)\n",
    "            conv_block_g(ni, nf), # (bs, nf, inp_sz, inp_sz)\n",
    "            up_block(), # (bs, nf, inp_sz*2, inp_sz*2)\n",
    "            ConvResBlockG(nf), # (bs, nf, inp_sz*2, inp_sz*2)\n",
    "            SelfAttention(nf) if is_self_attn else IdentityModule(), # (bs, nf, inp_sz*2, inp_sz*2)\n",
    "        )\n",
    "        self.to_rgb = to_rgb_block(nf) # (bs, 3, inp_sz*2, inp_sz*2)\n",
    "    def forward(self, code, word_emb, src_mask):\n",
    "        ''' code: (bs, ni, inp_sz, inp_sz), word_emb: (bs, seq_len, emb_sz), src_mask: (bs, seq_len) \n",
    "            returns: img(bs, 3, inp_sz*2, inp_sz*2), code(bs, nf, inp_sz*2, inp_sz*2) '''\n",
    "        code = self.conv(code, word_emb, src_mask) # (bs, nf, inp_sz*2, inp_sz*2)\n",
    "        img = self.to_rgb(code) # (bs, 3, inp_sz*2, inp_sz*2)\n",
    "        return img, code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_general_attn = G_General_Attn(25, 512, 256, is_self_attn=True)\n",
    "code = torch.randn(2, 512, 4, 4)\n",
    "word_emb = torch.randn(2, 2, 25)\n",
    "src_mask = torch.ones(2, 2).bool()\n",
    "img, code = g_general_attn(code, word_emb, src_mask)\n",
    "test_eq(img.shape, (2, 3, 8, 8))\n",
    "test_eq(code.shape, (2, 256, 8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Anime_G(nn.Module):\n",
    "    def __init__(self, emb_sz, noise_sz=100):\n",
    "        super().__init__()\n",
    "        self.g_init = G_Init(emb_sz, noise_sz) # (bs, 3, 4, 4), (bs, CH_TABLE[0], 4, 4)\n",
    "        self.gs = nn.ModuleList([\n",
    "            G_General(CH_TABLE[0], CH_TABLE[1]), # (bs, 3, 8, 8), (bs, CH_TABLE[1], 8, 8)\n",
    "            G_General(CH_TABLE[1], CH_TABLE[2]), # (bs, 3, 16, 16), (bs, CH_TABLE[2], 16, 16)\n",
    "        ])\n",
    "        self.gs_attn = nn.ModuleList([\n",
    "            G_General_Attn(emb_sz, CH_TABLE[2], CH_TABLE[3]), # (bs, 3, 32, 32), (bs, CH_TABLE[3], 32, 32)\n",
    "            G_General_Attn(emb_sz, CH_TABLE[3], CH_TABLE[4], is_self_attn=True), # (bs, 3, 64, 64), (bs, CH_TABLE[4], 64, 64)\n",
    "        ])\n",
    "    def forward(self, sent_emb, noise, word_emb, src_mask):\n",
    "        ''' sent_emb: (bs, emb_sz), noise: (bs, noise_sz), word_emb: (bs, seq_len, emb_sz), src_mask: (bs, seq_len) \n",
    "            returns: imgs: [(bs, 3, 4, 4),...,(bs, 3, 64, 64)] '''\n",
    "        imgs = []\n",
    "        img, code = self.g_init(sent_emb, noise) # (bs, 3, 4, 4), (bs, CH_TABLE[0], 4, 4)\n",
    "        imgs.append(img)\n",
    "        for g in self.gs:\n",
    "            img, code = g(code)\n",
    "            imgs.append(img)\n",
    "        for g in self.gs_attn:\n",
    "            img, code = g(code, word_emb, src_mask)\n",
    "            imgs.append(img)\n",
    "        return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_g = Anime_G(25, 100)\n",
    "sent_emb = torch.randn(2, 25)\n",
    "noise = torch.randn(2, 100)\n",
    "word_emb = torch.randn(2, 2, 25)\n",
    "src_mask = torch.ones(2, 2).bool()\n",
    "imgs = anime_g(sent_emb, noise, word_emb, src_mask)\n",
    "test_eq([img.shape for img in imgs], \n",
    "        [torch.Size([2, 3, 4, 4]), \n",
    "         torch.Size([2, 3, 8, 8]), \n",
    "         torch.Size([2, 3, 16, 16]), \n",
    "         torch.Size([2, 3, 32, 32]), \n",
    "         torch.Size([2, 3, 64, 64])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Birds_G(nn.Module):\n",
    "    def __init__(self, emb_sz, noise_sz=100):\n",
    "        super().__init__()\n",
    "        self.g_init = G_Init(emb_sz, noise_sz) # (bs, 3, 4, 4), (bs, CH_TABLE[0], 4, 4)\n",
    "        self.gs = nn.ModuleList([\n",
    "            G_General(CH_TABLE[0], CH_TABLE[1]), # (bs, 3, 8, 8), (bs, CH_TABLE[1], 8, 8)\n",
    "            G_General(CH_TABLE[1], CH_TABLE[2]), # (bs, 3, 16, 16), (bs, CH_TABLE[2], 16, 16)\n",
    "            G_General(CH_TABLE[2], CH_TABLE[3]), # (bs, 3, 32, 32), (bs, CH_TABLE[3], 32, 32)\n",
    "            G_General(CH_TABLE[3], CH_TABLE[4], is_self_attn=True), # (bs, 3, 64, 64), (bs, CH_TABLE[4], 64, 64)\n",
    "        ])\n",
    "        self.gs_attn = nn.ModuleList([\n",
    "            G_General_Attn(emb_sz, CH_TABLE[4], CH_TABLE[5]), # (bs, 3, 128, 128), (bs, CH_TABLE[5], 128, 128)\n",
    "            G_General_Attn(emb_sz, CH_TABLE[5], CH_TABLE[6]), # (bs, 3, 256, 256), (bs, CH_TABLE[6], 256, 256)\n",
    "        ])\n",
    "    def forward(self, sent_emb, noise, word_emb, src_mask):\n",
    "        ''' sent_emb: (bs, emb_sz), noise: (bs, noise_sz), word_emb: (bs, seq_len, emb_sz), src_mask: (bs, seq_len) \n",
    "            returns: imgs: [(bs, 3, 4, 4),...,(bs, 3, 256, 256)], code: (bs, 64, 256, 256)'''\n",
    "        imgs = []\n",
    "        img, code = self.g_init(sent_emb, noise) # (bs, 3, 4, 4), (bs, 512, 4, 4)\n",
    "        imgs.append(img)\n",
    "        for g in self.gs:\n",
    "            img, code = g(code)\n",
    "            imgs.append(img)\n",
    "        for g in self.gs_attn:\n",
    "            img, code = g(code, word_emb, src_mask)\n",
    "            imgs.append(img)\n",
    "        return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds_g = Birds_G(25, 100)\n",
    "sent_emb = torch.randn(2, 25)\n",
    "noise = torch.randn(2, 100)\n",
    "word_emb = torch.randn(2, 2, 25)\n",
    "src_mask = torch.ones(2, 2).bool()\n",
    "imgs = birds_g(sent_emb, noise, word_emb, src_mask)\n",
    "test_eq([img.shape for img in imgs], \n",
    "        [torch.Size([2, 3, 4, 4]), \n",
    "         torch.Size([2, 3, 8, 8]), \n",
    "         torch.Size([2, 3, 16, 16]), \n",
    "         torch.Size([2, 3, 32, 32]), \n",
    "         torch.Size([2, 3, 64, 64]),\n",
    "         torch.Size([2, 3, 128, 128]),\n",
    "         torch.Size([2, 3, 256, 256])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "class UncondCls(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cls = nn.Sequential(\n",
    "            conv2d(CH_TABLE[0]+1, 1, 4, 1, 0, bias=False), # (bs, 1, 1, 1)\n",
    "            nn.Flatten(), # (bs, 1)\n",
    "        )\n",
    "    def forward(self, sent_code):\n",
    "        ''' sent_code: (bs, CH_TABLE[0], 4, 4), returns: (bs, 1) '''\n",
    "        return self.cls(sent_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_code = torch.randn(2, CH_TABLE[0]+1, 4, 4)\n",
    "uncond_cls = UncondCls()\n",
    "uncond_logit = uncond_cls(sent_code)\n",
    "test_eq(uncond_logit.shape, (2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "class CondCls(nn.Module):\n",
    "    def __init__(self, emb_sz):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(4)\n",
    "        self.cls = nn.Sequential(\n",
    "            conv_block_d(CH_TABLE[0]+1+emb_sz, CH_TABLE[0]), # (bs, CH_TABLE[0], 4, 4)\n",
    "            conv2d(CH_TABLE[0], 1, 4, 1, 0, bias=False),\n",
    "            nn.Flatten(), # (bs, 1)\n",
    "        )\n",
    "    def forward(self, sent_code, sent_emb):\n",
    "        ''' sent_code: (bs, CH_TABLE[0], 4, 4), sent_emb: (bs, emb_sz)\n",
    "            returns: (bs, 1) '''\n",
    "        sent_emb = self.up(sent_emb[..., None, None]) # (bs, emb_sz, 4, 4)\n",
    "        sent_code = torch.cat([sent_code, sent_emb], 1) # (bs, emb_sz+CH_TABLE[0], 4, 4)\n",
    "        return self.cls(sent_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_code = torch.randn(2, CH_TABLE[0]+1, 4, 4)\n",
    "sent_emb = torch.randn(2, 25) \n",
    "cond_cls = CondCls(25)\n",
    "cond_logit = cond_cls(sent_code, sent_emb)\n",
    "test_eq(cond_logit.shape, (2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1024, 512, 256, 128, 64, 64, 64, 64, 64]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CH_TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "class D_General(nn.Module):\n",
    "    def __init__(self, ni, nf, is_first=False, is_self_attn=False):\n",
    "        super().__init__()\n",
    "        self.is_first = is_first\n",
    "        self.from_rgb = from_rgb_block(ni) # (bs, ni, inp_sz, inp_sz)\n",
    "        self.conv = nn.Sequential(\n",
    "            SelfAttention(ni) if is_self_attn else IdentityModule(), # (bs, ni, inp_sz, inp_sz)\n",
    "            ConvResBlockD(ni), # (bs, 2*ni, inp_sz, inp_sz)\n",
    "            conv_block_d(ni, nf), # (bs, nf, inp_sz, inp_sz)\n",
    "            down_block(), # (bs, nf, inp_sz//2, inp_sz//2)\n",
    "        )\n",
    "    def forward(self, img, code=None):\n",
    "        \"\"\" img: (bs, 3, inp_sz, inp_sz), code: (bs, ni, inp_sz, inp_sz)\n",
    "            returns: (bs, nf, inp_sz//2, inp_sz//2) \"\"\"\n",
    "        assert (code is None and self.is_first==True) or (code is not None and self.is_first==False)\n",
    "        code2 = self.from_rgb(img) # (bs, ni, inp_sz, inp_sz)\n",
    "        if self.is_first:\n",
    "            code = code2\n",
    "        else:\n",
    "            code = code + code2\n",
    "        code = self.conv(code) # (bs, nf, inp_sz//2, inp_sz//2)\n",
    "        return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_general = D_General(16, 32, is_self_attn=True)\n",
    "img = torch.randn(2, 3, 8, 8)\n",
    "code = torch.randn(2, 16, 8, 8)\n",
    "code = d_general(img, code)\n",
    "test_eq(code.shape, (2, 32, 4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Anime_D(nn.Module):\n",
    "    def __init__(self, emb_sz):\n",
    "        super().__init__()\n",
    "        self.ds = nn.ModuleList([\n",
    "            D_General(CH_TABLE[4], CH_TABLE[3], is_first=True, is_self_attn=True), # (bs, CH_TABLE[3], 32, 32)\n",
    "            D_General(CH_TABLE[3], CH_TABLE[2]), # (bs, CH_TABLE[2], 16, 16)\n",
    "            D_General(CH_TABLE[2], CH_TABLE[1]), # (bs, CH_TABLE[1], 8, 8)\n",
    "            D_General(CH_TABLE[1], CH_TABLE[0]), # (bs, CH_TABLE[0], 4, 4)\n",
    "        ])\n",
    "        self.minibatch_std_dev = MinibatchStdDev()\n",
    "        self.from_rgb = from_rgb_block(CH_TABLE[0])\n",
    "        self.uncond_cls = UncondCls()\n",
    "        self.cond_cls = CondCls(emb_sz)\n",
    "    def get_sent_code(self, imgs):\n",
    "        ''' imgs: [(2, 3, 64, 64), ..., (bs, 3, 4, 4)]\n",
    "            returns: (bs, CH_TABLE[0]+1, 4, 4) '''\n",
    "        assert len(imgs)==5\n",
    "        code = None\n",
    "        for i in range(len(self.ds)):\n",
    "            code = self.ds[i](imgs[i], code) # (bs, CH_TABLE[0], 4, 4)\n",
    "        code2 = self.from_rgb(imgs[-1]) # (bs, CH_TABLE[0], 4, 4)\n",
    "        sent_code = self.minibatch_std_dev(code + code2)\n",
    "        return sent_code\n",
    "    def forward(self, imgs, sent_emb):\n",
    "        ''' imgs: [(2, 3, 64, 64), ..., (bs, 3, 4, 4)], sent_emb: (bs, emb_sz)\n",
    "            returns: uncond_logit(bs, 1), cond_logit(bs, 1) '''\n",
    "        sent_code = self.get_sent_code(imgs) # (bs, CH_TABLE[0], 4, 4)\n",
    "        uncond_logit = self.uncond_cls(sent_code)\n",
    "        cond_logit = self.cond_cls(sent_code, sent_emb)\n",
    "        return uncond_logit, cond_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_d = Anime_D(25)\n",
    "imgs =[torch.randn(2, 3, 64, 64), torch.randn(2, 3, 32, 32), torch.randn(2, 3, 16, 16), torch.randn(2, 3, 8, 8), torch.randn(2, 3, 4, 4)]\n",
    "sent_emb = torch.randn(2, 25)\n",
    "\n",
    "sent_code = anime_d.get_sent_code(imgs)\n",
    "test_eq(sent_code.shape, (2, CH_TABLE[0]+1, 4, 4))\n",
    "\n",
    "uncond_logit, cond_logit = anime_d(imgs, sent_emb)\n",
    "test_eq(uncond_logit.shape, (2, 1))\n",
    "test_eq(cond_logit.shape, (2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Birds_D(nn.Module):\n",
    "    def __init__(self, emb_sz):\n",
    "        super().__init__()\n",
    "        self.ds = nn.ModuleList([\n",
    "            D_General(CH_TABLE[6], CH_TABLE[5], is_first=True), # (bs, CH_TABLE[5], 128, 128)\n",
    "            D_General(CH_TABLE[5], CH_TABLE[4]), # (bs, CH_TABLE[4], 64, 64)\n",
    "            D_General(CH_TABLE[4], CH_TABLE[3], is_self_attn=True), # (bs, CH_TABLE[3], 32, 32)\n",
    "            D_General(CH_TABLE[3], CH_TABLE[2]), # (bs, CH_TABLE[2], 16, 16)\n",
    "            D_General(CH_TABLE[2], CH_TABLE[1]), # (bs, CH_TABLE[1], 8, 8)\n",
    "            D_General(CH_TABLE[1], CH_TABLE[0]), # (bs, CH_TABLE[0], 4, 4)\n",
    "        ])\n",
    "        self.minibatch_std_dev = MinibatchStdDev()\n",
    "        self.from_rgb = from_rgb_block(CH_TABLE[0])\n",
    "        self.uncond_cls = UncondCls()\n",
    "        self.cond_cls = CondCls(emb_sz)\n",
    "    def get_sent_code(self, imgs):\n",
    "        ''' imgs: [(2, 3, 256, 256), ..., (bs, 3, 4, 4)]\n",
    "            returns: (bs, CH_TABLE[0]+1, 4, 4) '''\n",
    "        assert len(imgs)==7\n",
    "        code = None\n",
    "        for i in range(len(self.ds)):\n",
    "            code = self.ds[i](imgs[i], code) # (bs, CH_TABLE[0], 4, 4)\n",
    "        code2 = self.from_rgb(imgs[-1]) # (bs, CH_TABLE[0], 4, 4)\n",
    "        sent_code = self.minibatch_std_dev(code + code2)\n",
    "        return sent_code\n",
    "    def forward(self, imgs, sent_emb):\n",
    "        ''' imgs: [(2, 3, 256, 256), ..., (bs, 3, 4, 4)], sent_emb: (bs, emb_sz)\n",
    "            returns: uncond_logit(bs, 1), cond_logit(bs, 1) '''\n",
    "        sent_code = self.get_sent_code(imgs)\n",
    "        uncond_logit = self.uncond_cls(sent_code)\n",
    "        cond_logit = self.cond_cls(sent_code, sent_emb)\n",
    "        return uncond_logit, cond_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds_d = Birds_D(25)\n",
    "imgs =[torch.randn(2, 3, 256, 256), torch.randn(2, 3, 128, 128), torch.randn(2, 3, 64, 64), torch.randn(2, 3, 32, 32), torch.randn(2, 3, 16, 16), torch.randn(2, 3, 8, 8), torch.randn(2, 3, 4, 4)]\n",
    "sent_emb = torch.randn(2, 25)\n",
    "\n",
    "sent_code = birds_d.get_sent_code(imgs)\n",
    "test_eq(sent_code.shape, (2, CH_TABLE[0]+1, 4, 4))\n",
    "\n",
    "uncond_logit, cond_logit = birds_d(imgs, sent_emb)\n",
    "test_eq(uncond_logit.shape, (2, 1))\n",
    "test_eq(cond_logit.shape, (2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exported Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Anime_Export(nn.Module):\n",
    "    def __init__(self, vocab_sz, emb_sz, pad_id, rnn_layers=2, rnn_drop_p=0.5, noise_sz=100):\n",
    "        super().__init__()\n",
    "        self.noise_sz = noise_sz\n",
    "        self.rnn_encoder = RnnEncoder(vocab_sz, emb_sz, pad_id, n_layers=rnn_layers, drop_p=rnn_drop_p).requires_grad_(False).eval()\n",
    "        self.g_net = Anime_G(emb_sz, noise_sz)\n",
    "#         self.samples = ( torch.tensor([[1, 2],[3, 4]]), torch.tensor([2, 2]) ) # (input_ids, cap_len)\n",
    "    def forward(self, inp_ids, cap_len):\n",
    "        \"\"\" inp_ids: (bs, max_seq_len), cap_len: (bs,)  \n",
    "            returns: (bs, 3, 64, 64) \"\"\"\n",
    "        bs, max_seq_len = inp_ids.shape\n",
    "        device = inp_ids.device\n",
    "        src_mask = get_src_mask(cap_len, max_seq_len, device)\n",
    "        sent_emb, word_emb = self.rnn_encoder(inp_ids, cap_len)\n",
    "        noise = noise_gen.sample((bs, self.noise_sz)).to(device)\n",
    "        imgs = self.g_net(sent_emb, noise, word_emb, src_mask)\n",
    "        return imgs[-1]\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, path, device='cpu'):\n",
    "        state = torch.load(path, map_location=device)\n",
    "        vocab_sz = state['vocab_sz']\n",
    "        emb_sz = state['emb_sz']\n",
    "        pad_id = state['pad_id']\n",
    "        rnn_layers = state['rnn_layers']\n",
    "        rnn_drop_p = state['rnn_drop_p']\n",
    "        noise_sz = state['noise_sz']\n",
    "        m = cls(vocab_sz, emb_sz, pad_id, rnn_layers, rnn_drop_p, noise_sz)\n",
    "        m.rnn_encoder.load_state_dict(state['rnn_encoder'])\n",
    "        m.g_net.load_state_dict(state['g_net'])\n",
    "#         m.samples = state['samples']\n",
    "        return m\n",
    "#     def small_forward(self, inp_ids, cap_len):\n",
    "#         \"\"\" inp_ids: (bs, seq_len) for bs is small, cap_len: (bs,)\n",
    "#             returns: (bs+self.samples.shape[0], 3, 64, 64) \"\"\"\n",
    "#         inp_ids = torch.cat([inp_ids, self.samples[0]])\n",
    "#         cap_len = torch.cat([cap_len, self.samples[1]])\n",
    "#         return self(inp_ids, cap_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_export = Anime_Export(5, 24, 0)\n",
    "inp_ids = torch.tensor([[1, 2],\n",
    "                        [3, 4]])\n",
    "cap_len = torch.tensor([2, 2])\n",
    "img = anime_export(inp_ids, cap_len)\n",
    "test_eq(img.shape, (2, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anime_export = Anime_Export(5, 24, 0)\n",
    "# inp_ids = torch.tensor([[1, 2],\n",
    "#                         [3, 4]])\n",
    "# cap_len = torch.tensor([2, 2])\n",
    "# img = anime_export.small_forward(inp_ids, cap_len)\n",
    "# test_eq(img.shape, (anime_export.samples[0].shape[0]+2, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Birds_Export(nn.Module):\n",
    "    def __init__(self, vocab_sz, emb_sz, pad_id, rnn_layers=2, rnn_drop_p=0.5, noise_sz=100):\n",
    "        super().__init__()\n",
    "        self.noise_sz = noise_sz\n",
    "        self.rnn_encoder = RnnEncoder(vocab_sz, emb_sz, pad_id, n_layers=rnn_layers, drop_p=rnn_drop_p).requires_grad_(False).eval()\n",
    "        self.g_net = Birds_G(emb_sz, noise_sz)\n",
    "#         self.samples = ( torch.randint(0, 4, (2, 25)), torch.tensor( [8, 4]) ) # (input_ids, cap_len)\n",
    "    def forward(self, inp_ids, cap_len):\n",
    "        \"\"\" inp_ids: (bs, max_seq_len), cap_len: (bs,)  \n",
    "            returns: (bs, 3, 256, 256) \"\"\"\n",
    "        bs, max_seq_len = inp_ids.shape\n",
    "        device = inp_ids.device\n",
    "        src_mask = get_src_mask(cap_len, max_seq_len, device)\n",
    "        sent_emb, word_emb = self.rnn_encoder(inp_ids, cap_len)\n",
    "        noise = noise_gen.sample((bs, self.noise_sz)).to(device)\n",
    "        imgs = self.g_net(sent_emb, noise, word_emb, src_mask)\n",
    "        return imgs[-1]\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, path, device='cpu'):\n",
    "        state = torch.load(path, map_location=device)\n",
    "        vocab_sz = state['vocab_sz']\n",
    "        emb_sz = state['emb_sz']\n",
    "        pad_id = state['pad_id']\n",
    "        rnn_layers = state['rnn_layers']\n",
    "        rnn_drop_p = state['rnn_drop_p']\n",
    "        noise_sz = state['noise_sz']\n",
    "        m = cls(vocab_sz, emb_sz, pad_id, rnn_layers, rnn_drop_p, noise_sz)\n",
    "        m.rnn_encoder.load_state_dict(state['rnn_encoder'])\n",
    "        m.g_net.load_state_dict(state['g_net'])\n",
    "#         m.samples = state['samples']\n",
    "        return m\n",
    "#     def small_forward(self, inp_ids, cap_len):\n",
    "#         \"\"\" inp_ids: (bs, seq_len) for bs is small, cap_len: (bs,)\n",
    "#             returns: (bs+self.samples.shape[0], 3, 256, 256) \"\"\"\n",
    "#         inp_ids = torch.cat([inp_ids, self.samples[0]])\n",
    "#         cap_len = torch.cat([cap_len, self.samples[1]])\n",
    "#         return self(inp_ids, cap_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds_export = Birds_Export(5, 24, 0)\n",
    "inp_ids = torch.randint(0, 4, (2, 25))\n",
    "cap_len = torch.tensor([2, 2])\n",
    "img = birds_export(inp_ids, cap_len)\n",
    "test_eq(img.shape, (2, 3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# birds_export = Birds_Export(5, 24, 0)\n",
    "# inp_ids = torch.randint(0, 4, (2, 25))\n",
    "# cap_len = torch.tensor([2, 2])\n",
    "# img = birds_export.small_forward(inp_ids, cap_len)\n",
    "# test_eq(img.shape, (anime_export.samples[0].shape[0]+2, 3, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00a_torch_utils.ipynb.\n",
      "Converted 02a_data_anime_heads.ipynb.\n",
      "Converted 02b_data_birds.ipynb.\n",
      "Converted 03a_model.ipynb.\n",
      "Converted 04a_trainer_DAMSM.ipynb.\n",
      "Converted 04b_trainer_GAN-Copy1.ipynb.\n",
      "Converted 05a_inference_anime_heads.ipynb.\n",
      "Converted 05b_inference_birds.ipynb.\n",
      "Converted 99a_Train_Anime_Heads_DAMSM.ipynb.\n",
      "Converted 99b_Train_Anime_Heads_GAN.ipynb.\n",
      "Converted 99c_Train_Birds_DAMSM.ipynb.\n",
      "Converted 99d_Train_Birds_GAN.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
